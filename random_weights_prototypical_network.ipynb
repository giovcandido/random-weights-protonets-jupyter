{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GlyMLKngmf-t"
   },
   "source": [
    "# Prototypical Networks with Random Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jImaIF9I8lbf"
   },
   "source": [
    "## Download datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TVNdXdkjZayf"
   },
   "source": [
    "The mini-ImageNet dataset was provided by [Ren et al](https://arxiv.org/abs/1803.00676). They created the pickle files for train, validation and test. It can be found in their [few-shot-ssl-public](https://github.com/renmengye/few-shot-ssl-public) repository. But, in order to use the dataset, I copied the files to my drive, so it can be downloaded here: [mini-ImageNet](https://drive.google.com/uc?id=1GUDPuoH3JfbGR078vsuF5UFHuJTGXGFb&export=download).\n",
    "\n",
    "The splits follow the procedure of [Ravi and Larochelle](https://openreview.net/forum?id=rJY0-Kcll).\n",
    "\n",
    "The Omniglot dataset was provided by [Brenden Lake](https://github.com/brendenlake). However there were no pickle files that followed the exact procedure of [Vinyals et al](https://arxiv.org/abs/1606.04080) available. I decided to create the files myself using the split files provided by [Jake Snell](https://github.com/jakesnell/prototypical-networks).\n",
    "\n",
    "The dataset can be download here: [Omniglot](https://drive.google.com/u/0/uc?id=1ny3lCPETCLbcjSQHPc3aANUcNR82Aht5&export=download).\n",
    "\n",
    "Since the datasets are stored in Goodle Drive, I'll use gdown module to download them automatically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b0B8vAA6ChAn"
   },
   "source": [
    "First, let's **install** gdown module using pip3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7aV-Lqty3qTu",
    "outputId": "746316e8-3df6-466a-fedd-5be571c3da44"
   },
   "outputs": [],
   "source": [
    "!pip3 install gdown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Omniglot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a directory to store Omniglot data and results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p omniglot\n",
    "!mkdir -p omniglot/data\n",
    "\n",
    "data_dir = \"omniglot/data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EoHL-hD7H0Ha"
   },
   "source": [
    "**Download** the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OkglTOwt6BWV",
    "outputId": "763d0219-b94f-41b2-82f7-513c6e930021"
   },
   "outputs": [],
   "source": [
    "!gdown -O $data_dir/omniglot.tar.gz \"https://drive.google.com/u/0/uc?id=1ny3lCPETCLbcjSQHPc3aANUcNR82Aht5&export=download\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tqhULVfkD_MU"
   },
   "source": [
    "**Extract** the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y5o1Fz2xEFVJ",
    "outputId": "9fcbeb70-37e3-4593-c382-5619f665dce7"
   },
   "outputs": [],
   "source": [
    "!tar -xvf $data_dir/omniglot.tar.gz -C $data_dir\n",
    "!rm -f $data_dir/omniglot.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mini-ImageNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a directory to store mini-ImageNet data and results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p mini_imagenet\n",
    "!mkdir -p mini_imagenet/data\n",
    "\n",
    "data_dir = \"mini_imagenet/data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EoHL-hD7H0Ha"
   },
   "source": [
    "**Download** the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OkglTOwt6BWV",
    "outputId": "763d0219-b94f-41b2-82f7-513c6e930021"
   },
   "outputs": [],
   "source": [
    "!gdown -O $data_dir/miniImageNet.tar.gz \"https://drive.google.com/uc?id=1GUDPuoH3JfbGR078vsuF5UFHuJTGXGFb&export=download\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tqhULVfkD_MU"
   },
   "source": [
    "**Extract** the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y5o1Fz2xEFVJ",
    "outputId": "9fcbeb70-37e3-4593-c382-5619f665dce7"
   },
   "outputs": [],
   "source": [
    "!tar -xvf $data_dir/miniImageNet.tar.gz -C $data_dir\n",
    "!rm -f $data_dir/miniImageNet.tar.gz\n",
    "\n",
    "!mv $data_dir/mini-imagenet-cache-train.pkl $data_dir/train.pkl\n",
    "!mv $data_dir/mini-imagenet-cache-val.pkl $data_dir/valid.pkl\n",
    "!mv $data_dir/mini-imagenet-cache-test.pkl $data_dir/test.pkl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qbJTv_8XBCKF"
   },
   "source": [
    "## Image Loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pSvYg_8cZOtW"
   },
   "source": [
    "Here we are loading the images from the pkl files to the respective lists.\n",
    "\n",
    "Each file correspond to a image set.\n",
    "\n",
    "For each set, we're going to create two lists: one for the images (the x) and one for the labels (the y)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jdahdod76aQs"
   },
   "source": [
    "**Import** some modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kK9yGcsx5X6Z"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "from tqdm.notebook import trange, tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zqxlBrT-6g3a"
   },
   "source": [
    "**Function** to load images and their labels from a pkl file. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ptZ5lGYbQbQB"
   },
   "outputs": [],
   "source": [
    "# file_name -> path + name of the file\n",
    "def load_images(file_name):\n",
    "    # get file content\n",
    "    with open(file_name, 'rb') as f:\n",
    "        info = pickle.load(f)\n",
    "\n",
    "    img_data = info['image_data']\n",
    "    class_dict = info['class_dict']\n",
    "\n",
    "    # create arrays to store x and y of images\n",
    "    images = [] # x\n",
    "    labels = [] # y\n",
    "  \n",
    "    # loop over all images and store them\n",
    "    loading_msg = 'Reading images from %s' % file_name\n",
    "\n",
    "    # loop over all classes\n",
    "    for item in tqdm(class_dict.items(), desc = loading_msg):\n",
    "        # loop over all examples from the class\n",
    "        for example_num in item[1]:\n",
    "            # convert image to RGB color channels\n",
    "            RGB_img = cv2.cvtColor(img_data[example_num], cv2.COLOR_BGR2RGB)\n",
    "\n",
    "            # store image and corresponding label\n",
    "            images.append(RGB_img)\n",
    "            labels.append(item[0])\n",
    "  \n",
    "    # return set of images\n",
    "    return np.array(images), np.array(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J2VcplJ3xA-z"
   },
   "source": [
    "## Episode extractor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o3xWaO96sN3p"
   },
   "source": [
    "**Import** some modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hnvn14Q5Pqkz"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eXwttd2zsV8d"
   },
   "source": [
    "**Function** to generate one episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IOTBxVVwOdxC"
   },
   "outputs": [],
   "source": [
    "# img_set_x -> images\n",
    "# img_set_y -> labels\n",
    "# num_way -> number of classes for episode\n",
    "# num_shot -> number of examples per class\n",
    "# num_query -> number of query examples per class \n",
    "def extract_episode(img_set_x, img_set_y, num_way, num_shot, num_query):\n",
    "    # get a list of all unique labels (no repetition)\n",
    "    unique_labels = np.unique(img_set_y)\n",
    "\n",
    "    # select num_way classes randomly without replacement\n",
    "    chosen_labels = np.random.choice(unique_labels, num_way, replace = False)\n",
    "    # number of examples per selected class (label)\n",
    "    examples_per_label = num_shot + num_query\n",
    "\n",
    "    # list to store the episode\n",
    "    episode = []\n",
    "\n",
    "    # iterate over all selected labels \n",
    "    for label_l in chosen_labels:\n",
    "        # get all images with a certain label l\n",
    "        images_with_label_l = img_set_x[img_set_y == label_l]\n",
    "\n",
    "        # suffle images with label l\n",
    "        shuffled_images = np.random.permutation(images_with_label_l)\n",
    "\n",
    "        # chose examples_per_label images with label l\n",
    "        chosen_images = shuffled_images[:examples_per_label]\n",
    "\n",
    "        # add the chosen images to the episode\n",
    "        episode.append(chosen_images)\n",
    "\n",
    "    # turn python list into a numpy array\n",
    "    episode = np.array(episode)\n",
    "\n",
    "    # convert numpy array to tensor of floats\n",
    "    episode = torch.from_numpy(episode).float()\n",
    "\n",
    "    # reshape tensor (required)\n",
    "    episode = episode.permute(0,1,4,2,3)\n",
    "\n",
    "    # get the shape of the images\n",
    "    img_dim = episode.shape[2:]\n",
    "  \n",
    "    # build a dict with info about the generated episode\n",
    "    episode_dict = {\n",
    "        'images': episode, 'num_way': num_way, 'num_shot': num_shot, \n",
    "        'num_query': num_query, 'img_dim': img_dim}\n",
    "\n",
    "    return episode_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O-L91GnwshPO"
   },
   "source": [
    "**Function** to display a grid representation of an episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ia3JLpQRu4fh"
   },
   "outputs": [],
   "source": [
    "# episode_dict -> dict with info about the chosen episode\n",
    "def display_episode_images(episode_dict):\n",
    "    # number of examples per class \n",
    "    examples_per_label = episode_dict['num_shot'] + episode_dict['num_query']\n",
    "\n",
    "    # total number of images\n",
    "    num_images = episode_dict['num_way'] * examples_per_label\n",
    "\n",
    "    # select the images\n",
    "    images = episode_dict['images'].view(num_images, *episode_dict['img_dim'])\n",
    "\n",
    "    # create a grid with all the images\n",
    "    grid_img = torchvision.utils.make_grid(images, nrow = examples_per_label)\n",
    "\n",
    "    # reshape the tensor and convert to numpy array of integers \n",
    "    grid_img = grid_img.permute(1, 2, 0).numpy().astype(np.uint8)\n",
    "\n",
    "    # chage image from BGR to RGB\n",
    "    grid_img = cv2.cvtColor(grid_img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # set a bigger size\n",
    "    plt.figure(figsize = (80, 8))\n",
    "\n",
    "    # remove the axis\n",
    "    plt.axis('off')\n",
    "\n",
    "    # plot the grid image\n",
    "    plt.imshow(grid_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1RVjD3LrSfqf"
   },
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import** required modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "\n",
    "from os import path, mkdir, remove"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0rQyOFMPildk"
   },
   "source": [
    "Here we have some utility functions like the metric used in the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wiJ8ffJxctfD"
   },
   "source": [
    "### Euclidean Distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CMZnz22QcMmx"
   },
   "source": [
    "Metric used to determine the distance between all query points and prototypes of an episode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ey9flzIUjilR"
   },
   "source": [
    "**Some explanations**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uv9rHxkIjcX-"
   },
   "source": [
    "Consider a 20-way 5-shot problem with 15 query images per class. </br>\n",
    "Suppose x (query embeddings) has a size [300, 1600] and y (prototypes) [20, 1600].\n",
    "\n",
    "The unsqueeze function calls will transform the sizes of x and y. So, x will have a size [300, 1, 1600] and y [1, 20, 1600]. </br>\n",
    "Then, the expand function calls will cause x to have a size [300, 20, 1600] and y a size [300, 20, 1600].\n",
    "\n",
    "These transformations are required since x and y will be subtracted in pow function call. </br>\n",
    "In other words, they need to have the same shape in order to calculate the distance.\n",
    "\n",
    "Once they are in the same shape, pow function is called. </br>\n",
    "The sum function will return a [300, 20] tensor with all the distances. \n",
    "\n",
    "Example: [i][j] is the distance between query embedding i and prototype j."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tZuUj_7-joG5"
   },
   "source": [
    "**Function** to calculate the euclidean distance between embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tjf4lqBFB_Ei"
   },
   "outputs": [],
   "source": [
    "def euclidean_dist(x, y):\n",
    "    elements_in_x = x.size(0)\n",
    "    elements_in_y = y.size(0)\n",
    "\n",
    "    dimension_elements = x.size(1)\n",
    "\n",
    "    assert dimension_elements == y.size(1)\n",
    "\n",
    "    x = x.unsqueeze(1).expand(elements_in_x , elements_in_y, dimension_elements)\n",
    "    y = y.unsqueeze(0).expand(elements_in_x , elements_in_y, dimension_elements)\n",
    "\n",
    "    distances = torch.pow(x - y, 2).sum(2)\n",
    "\n",
    "    return distances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's **configure** the logger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "console_logging_format = \"%(message)s\"\n",
    "\n",
    "logging.basicConfig(stream = sys.stdout, level = logging.INFO, format = console_logging_format)\n",
    "\n",
    "print(logging.getLogger().handlers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Function** to remove old and add new handlers to logger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_logger(log_dir, file_name):\n",
    "    # get logger\n",
    "    logger = logging.getLogger()\n",
    "    \n",
    "    # remove previous handlers, if they exist\n",
    "    if bool(logger.handlers):\n",
    "        logger.handlers.clear()\n",
    "    \n",
    "    # create a log directory, if not exists\n",
    "    if not path.exists(log_dir):\n",
    "        mkdir(log_dir)\n",
    "    \n",
    "    log_file_path = path.join(log_dir, file_name)\n",
    "    \n",
    "    # remove old log file (w/ same name)\n",
    "    if path.exists(log_file_path):\n",
    "        remove(log_file_path)\n",
    "    \n",
    "    # create a new log file\n",
    "    f = open(log_file_path, 'w+')\n",
    "    f.close()\n",
    "\n",
    "    # create a file handler for output file\n",
    "    file_handler = logging.FileHandler(log_file_path)\n",
    "    file_handler.setLevel(logging.INFO)\n",
    "    \n",
    "    # configure message to log in file\n",
    "    file_logging_format = \"[%(levelname)s] %(asctime)s: %(message)s\"\n",
    "    formatter = logging.Formatter(file_logging_format)\n",
    "    file_handler.setFormatter(formatter)\n",
    "    \n",
    "    # create a console handler\n",
    "    console_handler = logging.StreamHandler(sys.stdout)\n",
    "    console_handler.setLevel(logging.INFO)\n",
    "    \n",
    "    # configure message to log in console\n",
    "    console_logging_format = \"%(message)s\"\n",
    "    formatter = logging.Formatter(console_logging_format)\n",
    "    console_handler.setFormatter(formatter)\n",
    "    \n",
    "    # add handlers to logger\n",
    "    logger.addHandler(file_handler)\n",
    "    logger.addHandler(console_handler)\n",
    "    \n",
    "    return logger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to generate an array of random weights with size = size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Some explanations**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, a list from 1 to size * 10 is generated. Then, this list is converted to a numpy array. </br>\n",
    "From this array, a subarray with size = size is randomly extracted.\n",
    "\n",
    "Each element is divided by the sum of the subarray. A new subarray is obtained. </br>\n",
    "The sum of the new subarray is 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_weights(size):\n",
    "    upper_bound = size * 10\n",
    "    \n",
    "    values_to_pick_from = list(range(1, upper_bound + 1))\n",
    "    values_to_pick_from = np.array(values_to_pick_from)\n",
    "\n",
    "    chosen_values = np.random.choice(values_to_pick_from, size, replace = False)\n",
    "    chosen_values_sum = sum(chosen_values)\n",
    "\n",
    "    return [(x / chosen_values_sum) for x in chosen_values]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HbakTT_nFubI"
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import** required modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.autograd import Variable\n",
    "from os import path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model below uses a **weigthed average of the support embeddings**.\n",
    "\n",
    "For each class, weights are randomly generated. One weight for each support embedding. </br>\n",
    "The sum of the weights of a class add up to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Flatten, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x.view(x.size(0), -1)\n",
    "\n",
    "class ProtoNetWithRandomWeights(nn.Module):\n",
    "    def __init__(self, encoder):\n",
    "        super(ProtoNetWithRandomWeights, self).__init__()\n",
    "        self.encoder = encoder.cuda()\n",
    "\n",
    "    def set_forward_loss(self, dataset, episode_dict):\n",
    "        # extract all images \n",
    "        images = episode_dict['images'].cuda()\n",
    "        \n",
    "        # get episode setup\n",
    "        num_way = episode_dict['num_way'] # way\n",
    "        num_shot = episode_dict['num_shot'] # shot\n",
    "        num_query = episode_dict['num_query'] # number of query images\n",
    "        \n",
    "        # from each class, extract num_shot support images\n",
    "        x_support = images[:, :num_shot] # lines are classes and columns are images\n",
    "        \n",
    "        # from each class, extract the remaining images as query images\n",
    "        x_query = images[:, num_shot:] # lines are classes and columns are images\n",
    "\n",
    "        # create indices from 0 to num_way-1 for classification\n",
    "        target_inds = torch.arange(0, num_way).view(num_way, 1, 1)\n",
    "        \n",
    "        # replicate all indices num_query times (for each query image)\n",
    "        target_inds = target_inds.expand(num_way, num_query, 1).long()\n",
    "        \n",
    "        # convert indices from Tensor to Variable\n",
    "        target_inds = Variable(target_inds, requires_grad = False).cuda()\n",
    "        \n",
    "        # transform x_support into a array in which all images are contiguous\n",
    "        x_support = x_support.contiguous().view(\n",
    "            num_way * num_shot, *x_support.size()[2:]) # no more lines and columns\n",
    "                \n",
    "        # transform x_query into a array in which all images are contiguous\n",
    "        x_query = x_query.contiguous().view(\n",
    "            num_way * num_query, *x_query.size()[2:]) # no more lines and columns\n",
    "\n",
    "        # join all images into a single contiguous array \n",
    "        x = torch.cat([x_support, x_query], 0)\n",
    "        \n",
    "        # encode all images\n",
    "        z = self.encoder.forward(x) # embeddings\n",
    "        \n",
    "        weights_file = path.join(dataset, 'weights.pkl')\n",
    "        \n",
    "        if path.exists(weights_file):\n",
    "            with open(weights_file, 'rb') as f:\n",
    "                # retrieve weights already generated\n",
    "                weights = pickle.load(f)\n",
    "        else:\n",
    "            # generate random weights\n",
    "            weights = generate_random_weights(num_shot)\n",
    "            \n",
    "            with open(weights_file, 'wb') as f:\n",
    "                pickle.dump(weights, f)\n",
    "            \n",
    "        # for each class i\n",
    "        for i in range(0, num_way):             \n",
    "            # index of the first support embedding\n",
    "            start = i * num_shot\n",
    "            \n",
    "            # index of the last support embedding is end-1\n",
    "            end = start + num_shot\n",
    "            \n",
    "            # index for the weight array\n",
    "            k = 0\n",
    "            \n",
    "            # for each support embedding\n",
    "            for j in range(start, end):\n",
    "                # multiply the embedding by its respective weight\n",
    "                z[j] = torch.mul(z[j], weights[k])\n",
    "                \n",
    "                k += 1\n",
    "        \n",
    "        # compute class prototypes\n",
    "        z_dim = z.size(-1)\n",
    "        z_proto = z[:(num_way * num_shot)].view(num_way, num_shot, z_dim).sum(1)\n",
    "        \n",
    "        # get the query embeddings\n",
    "        z_query = z[(num_way * num_shot):]\n",
    "\n",
    "        # compute distance between query embeddings and class prototypes\n",
    "        dists = euclidean_dist(z_query, z_proto)\n",
    "        \n",
    "        # compute the log probabilities\n",
    "        log_p_y = F.log_softmax(-dists, dim = 1).view(num_way, num_query, -1)\n",
    "        \n",
    "        # compute the loss\n",
    "        loss_val = -log_p_y.gather(2, target_inds).squeeze().view(-1).mean()\n",
    "        \n",
    "        # get the predicted labels for each query\n",
    "        _, y_hat = log_p_y.max(2) # lines are classes and columns are query embeddings\n",
    "        \n",
    "        # compute the accuracy\n",
    "        acc_val = torch.eq(y_hat, target_inds.squeeze()).float().mean()\n",
    "        \n",
    "        # return output: loss, acc and predicted value\n",
    "        return loss_val, {\n",
    "            'loss': loss_val.item(), 'acc': acc_val.item(), 'y_hat': y_hat}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Function** to load the model with random weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to load the model structure\n",
    "def load_protonet_with_random_weights(x_dim, hid_dim, z_dim):\n",
    "    # define a convolutional block\n",
    "    def conv_block(layer_input, layer_output): \n",
    "        conv = nn.Sequential(\n",
    "            nn.Conv2d(layer_input, layer_output, 3, padding=1),\n",
    "            nn.BatchNorm2d(layer_output), nn.ReLU(), \n",
    "            nn.MaxPool2d(2))\n",
    "\n",
    "        return conv\n",
    "  \n",
    "    # create the encoder to the embeddings for the images\n",
    "    # the encoder is made of four conv blocks \n",
    "    encoder = nn.Sequential(\n",
    "        conv_block(x_dim[0], hid_dim), conv_block(hid_dim, hid_dim), \n",
    "        conv_block(hid_dim, hid_dim), conv_block(hid_dim, z_dim), Flatten())\n",
    "  \n",
    "    return ProtoNetWithRandomWeights(encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ssBp1OrZ17oC"
   },
   "source": [
    "## For training and evaluating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tcSV0p3E1pvG"
   },
   "source": [
    "**Import** required modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "olSQt0vGg_T3"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np\n",
    "from math import fsum\n",
    "\n",
    "from tqdm.notebook import trange\n",
    "from os import path, mkdir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Choose** a dataset to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Options: \"omniglot\" or \"mini_imagenet\"\n",
    "dataset = \"mini_imagenet\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load** train, validation and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = path.join(dataset, 'data')\n",
    "\n",
    "train_x, train_y = load_images(path.join(data_dir,  'train.pkl'))\n",
    "valid_x, valid_y = load_images(path.join(data_dir,  'valid.pkl'))\n",
    "test_x, test_y = load_images(path.join(data_dir,  'test.pkl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E0R2trhfWtgt"
   },
   "source": [
    "We can **check** the shape of our sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y7f28KKKW6tg"
   },
   "source": [
    "The shape will be ((n, w, d, c), (n,)), where:\n",
    "* n is the number of images and corresponding labels;\n",
    "* w is the width of the images;\n",
    "* h is the height of the images;\n",
    "* c is the number of color channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k9lw2RQ1Wf5Z",
    "outputId": "cfd1c2ec-e1d5-41d5-9496-11adef3cf586"
   },
   "outputs": [],
   "source": [
    "train_x.shape, train_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KlxqU4ZuXXrE",
    "outputId": "4af04201-2bd0-49a2-dbf3-5b6a999d95e3"
   },
   "outputs": [],
   "source": [
    "valid_x.shape, valid_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RGln1ravXb8l",
    "outputId": "2393bfa8-5ec0-4e39-bd38-58e3c37b9835"
   },
   "outputs": [],
   "source": [
    "test_x.shape, test_y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generate** and **display** an episode to check if everything is okay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_dict = extract_episode(train_x, train_y, num_way = 5, num_shot = 5, num_query = 5)\n",
    "\n",
    "display_episode_images(episode_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "au_4d_omPYxB"
   },
   "source": [
    "**Create** a directory to store the time of execution and the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7DHl1ie_PeBd"
   },
   "outputs": [],
   "source": [
    "results_dir = path.join(dataset, 'results')\n",
    "\n",
    "if not path.exists(results_dir):\n",
    "    mkdir(results_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2LB6fEIxKpS2"
   },
   "source": [
    "**Set** hyperparameters and other parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8tUtxYAWK_VD"
   },
   "outputs": [],
   "source": [
    "best_epoch = {\n",
    "    'number': -1,\n",
    "    'loss': np.inf,\n",
    "    'acc': 0}\n",
    "\n",
    "config = {\n",
    "    'dataset': dataset,\n",
    "    'results_dir': results_dir,\n",
    "    'learning_rate': 0.001,\n",
    "    'decay_every': 20,\n",
    "    'patience': 200,\n",
    "    'best_epoch': best_epoch,\n",
    "    'wait': 0,\n",
    "    'stop': False}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load** model structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# images have 3 color channels and are 84 x 84\n",
    "# Convolutional blocks have 64 filters\n",
    "model = load_protonet_with_random_weights(x_dim  = (3, 84, 84), hid_dim = 64, z_dim = 64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want it, you can **delete** the weights file to generate them again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -f $dataset/'weights.pkl'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Fke38yY6pI9"
   },
   "source": [
    "## Train and evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JwzSpmETvNVQ"
   },
   "source": [
    "**Function** to train the model on the train set through many epochs.\n",
    "\n",
    "At the end of each epoch, the model is evaluated on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BnLOgECOKG_y"
   },
   "outputs": [],
   "source": [
    "# model -> model structure\n",
    "# config -> dict with some hyperparameters and important configs\n",
    "# train_data -> train set, num_way, num_shot, num_query etc\n",
    "# valid_data -> validation set, num_way, num_shot, num_query etc\n",
    "def train(model, config, train_data, valid_data, logger):\n",
    "    # set Adam optimizer with an initial learning rate\n",
    "    optimizer = optim.Adam(\n",
    "        model.parameters(), lr = config['learning_rate'])\n",
    "\n",
    "    # schedule learning rate to be cut in half every 2000 episodes\n",
    "    scheduler = optim.lr_scheduler.StepLR(\n",
    "        optimizer, config['decay_every'], gamma = 0.5, last_epoch = -1)\n",
    "\n",
    "    # set model to training mode\n",
    "    model.train()\n",
    "\n",
    "    # number of epochs so far\n",
    "    epochs_so_far = 0\n",
    "\n",
    "    # train until early stopping says so\n",
    "    # or until the max number of epochs is not achived \n",
    "    while epochs_so_far < train_data['max_epoch'] and not config['stop']:\n",
    "        epoch_loss = 0.0\n",
    "        epoch_acc = 0.0\n",
    "\n",
    "        logger.info('==> Epoch %d' % (epochs_so_far + 1))\n",
    "\n",
    "        logger.info('> Training')\n",
    "\n",
    "        # do epoch_size classification tasks to train the model\n",
    "        for episode in trange(train_data['epoch_size']):\n",
    "            # get the episode dict\n",
    "            episode_dict = extract_episode(\n",
    "              train_data['train_x'], train_data['train_y'], train_data['num_way'], \n",
    "              train_data['num_shot'], train_data['num_query'])\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # classify images and get the loss and the acc of the curr episode\n",
    "            loss, output = model.set_forward_loss(config['dataset'], episode_dict)\n",
    "\n",
    "            # acumulate the loss and the acc\n",
    "            epoch_loss += output['loss']\n",
    "            epoch_acc += output['acc']\n",
    "\n",
    "            # update the model parameters (weights and biases)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # average the loss and the acc to get the epoch loss and the acc\n",
    "        epoch_loss = epoch_loss / train_data['epoch_size']\n",
    "        epoch_acc = epoch_acc / train_data['epoch_size']\n",
    "\n",
    "        # output the epoch loss and the epoch acc\n",
    "        logger.info('Loss: %.4f / Acc: %.2f%%' % (epoch_loss, (epoch_acc * 100)))\n",
    "\n",
    "        # do one epoch of evaluation on the validation test\n",
    "        evaluate_valid(model, config, valid_data, epochs_so_far + 1, logger)\n",
    "\n",
    "        # increment the number of epochs\n",
    "        epochs_so_far += 1\n",
    "\n",
    "        # tell the scheduler to increment its counter\n",
    "        scheduler.step()\n",
    "\n",
    "    # get dict with info about the best epoch\n",
    "    best_epoch = config['best_epoch']\n",
    "\n",
    "    # at the end of the training, output the best loss and the best acc\n",
    "    logger.info('Best loss: %.4f / Best Acc: %.2f%%' \n",
    "          % (best_epoch['loss'], (best_epoch['acc'] * 100)))\n",
    "\n",
    "    # save dict with info about the best epoch\n",
    "    with open(path.join(config['results_dir'], 'best_epoch.pkl'), 'wb') as f:\n",
    "        pickle.dump(best_epoch, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ONjkyKeUvngJ"
   },
   "source": [
    "**Function** to evaluate the model on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qCgoOcCaERHH"
   },
   "outputs": [],
   "source": [
    "# model -> model structure\n",
    "# config -> dict with some hyperparameters and important configs\n",
    "# valid_data -> validation set, num_way, num_shot, num_query etc\n",
    "# epoch -> number of the respective training epoch\n",
    "def evaluate_valid(model, config, valid_data, curr_epoch, logger):\n",
    "    # set model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    valid_loss = 0.0\n",
    "    valid_acc = 0.0\n",
    "\n",
    "    logger.info('> Validation')\n",
    "\n",
    "    # do epoch_size classification tasks to evaluate the model\n",
    "    for episode in trange(valid_data['epoch_size']):\n",
    "        # get the episode dict\n",
    "        episode_dict = extract_episode(\n",
    "            valid_data['valid_x'], valid_data['valid_y'], valid_data['num_way'], \n",
    "            valid_data['num_shot'], valid_data['num_query'])\n",
    "\n",
    "        # classify images and get the loss and the acc of the curr episode\n",
    "        _, output = model.set_forward_loss(config['dataset'], episode_dict)\n",
    "\n",
    "        # acumulate the loss and the acc\n",
    "        valid_loss += output['loss']\n",
    "        valid_acc += output['acc']\n",
    "  \n",
    "    # average the loss and the acc to get the valid loss and the acc\n",
    "    valid_loss = valid_loss / valid_data['epoch_size']\n",
    "    valid_acc = valid_acc / valid_data['epoch_size']\n",
    "\n",
    "    # output the valid loss and the valid acc\n",
    "    logger.info('Loss: %.4f / Acc: %.2f%%' % (valid_loss, (valid_acc * 100)))\n",
    "\n",
    "    # implement early stopping mechanism\n",
    "    # check if valid_loss is the best so far\n",
    "    if config['best_epoch']['loss'] > valid_loss:\n",
    "        # if true, save the respective train epoch\n",
    "        config['best_epoch']['number'] = curr_epoch\n",
    "\n",
    "        # save the best loss and the respective acc\n",
    "        config['best_epoch']['loss'] = valid_loss\n",
    "        config['best_epoch']['acc'] = valid_acc\n",
    "\n",
    "        # save the model with the best loss so far\n",
    "        model_file = path.join(config['results_dir'], 'best_model.pth')\n",
    "        torch.save(model.state_dict(), model_file)\n",
    "\n",
    "        logger.info('=> This is the best model so far! Saving...')\n",
    "\n",
    "        # set wait to zero\n",
    "        config['wait'] = 0\n",
    "    else:\n",
    "        # if false, increment the wait\n",
    "        config['wait'] += 1\n",
    "\n",
    "        # when the wait is bigger than the patience\n",
    "        if config['wait'] > config['patience']:\n",
    "            # the train has to stop\n",
    "            config['stop'] = True\n",
    "\n",
    "            logger.info('Patience was exceeded... Stopping...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aHBFDYWtv2iT"
   },
   "source": [
    "Now, let's **train** the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7uMy99qE-Ed6"
   },
   "outputs": [],
   "source": [
    "if dataset == 'mini_imagenet':\n",
    "    train_data = {\n",
    "        'train_x': train_x,\n",
    "        'train_y': train_y,\n",
    "        'num_way': 20,\n",
    "        'num_shot': 5,\n",
    "        'num_query': 15,\n",
    "        'max_epoch': 10000,\n",
    "        'epoch_size': 100}\n",
    "elif dataset == 'omniglot':\n",
    "    train_data = {\n",
    "        'train_x': train_x,\n",
    "        'train_y': train_y,\n",
    "        'num_way': 60,\n",
    "        'num_shot': 5,\n",
    "        'num_query': 5,\n",
    "        'max_epoch': 10000,\n",
    "        'epoch_size': 100}\n",
    "\n",
    "valid_data = {\n",
    "    'valid_x': valid_x,\n",
    "    'valid_y': valid_y,\n",
    "    'num_way': 5,\n",
    "    'num_shot': 5,\n",
    "    'num_query': 15,\n",
    "    'epoch_size': 100}\n",
    "\n",
    "train_logger = create_logger(path.join(dataset, 'logs'), 'train.log')\n",
    "\n",
    "train_time = %timeit -n1 -r1 -q -o train(model, config, train_data, valid_data, train_logger)\n",
    "\n",
    "train_logger.info('Time taken: ' + str(train_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jRwWsxTPv86y"
   },
   "source": [
    "**Check** best epoch number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l-ETi9djurJ-"
   },
   "outputs": [],
   "source": [
    "with open(path.join(config['results_dir'], 'best_epoch.pkl'), 'rb') as f:\n",
    "    number = pickle.load(f)['number']\n",
    "\n",
    "print(number)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Q8yqvfEnWG5"
   },
   "source": [
    "## Train with train + valid sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5UMtSiGbwAMa"
   },
   "source": [
    "**Function** to retrain the model using both train and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0qykJQfmnbOr"
   },
   "outputs": [],
   "source": [
    "# model -> model structure\n",
    "# config -> dict with some hyperparameters and important configs\n",
    "# retrain_data -> train + valid set, num_way, num_shot, num_query etc\n",
    "def retrain(model, config, retrain_data, logger):\n",
    "    # load the saved model\n",
    "    state_dict = torch.load(path.join(config['results_dir'], 'best_model.pth'))\n",
    "    model.load_state_dict(state_dict)\n",
    "\n",
    "    # set Adam optimizer with an initial learning rate\n",
    "    optimizer = optim.Adam(\n",
    "      model.parameters(), lr = config['learning_rate'])\n",
    "\n",
    "    # schedule learning rate to be cut in half every 2000 episodes\n",
    "    scheduler = optim.lr_scheduler.StepLR(\n",
    "        optimizer, config['decay_every'], gamma = 0.5, last_epoch = -1)\n",
    "\n",
    "    # set model to training mode\n",
    "    model.train()\n",
    "\n",
    "    # number of epochs so far\n",
    "    epochs_so_far = 0\n",
    "\n",
    "    # retrieve the epoch with best valid loss to determine max_epoch\n",
    "    with open(path.join(config['results_dir'], 'best_epoch.pkl'), 'rb') as f:\n",
    "        best_epoch_num = pickle.load(f)['number']\n",
    "  \n",
    "    max_epoch = best_epoch_num + 1#config['patience']\n",
    "  \n",
    "    while epochs_so_far < max_epoch:\n",
    "        epoch_loss = 0.0\n",
    "        epoch_acc = 0.0\n",
    "\n",
    "        logger.info('==> Epoch %d' % (epochs_so_far + 1))\n",
    "\n",
    "        # do epoch_size classification tasks to evaluate the model\n",
    "        for episode in trange(retrain_data['epoch_size']):\n",
    "            # get the episode dict\n",
    "            episode_dict = extract_episode(\n",
    "                retrain_data['retrain_x'], retrain_data['retrain_y'], \n",
    "                retrain_data['num_way'], retrain_data['num_shot'], \n",
    "                retrain_data['num_query'])\n",
    "      \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # classify images and get the loss and the acc of the curr episode\n",
    "            loss, output = model.set_forward_loss(config['dataset'], episode_dict)\n",
    "\n",
    "            # acumulate the loss and the acc\n",
    "            epoch_loss += output['loss']\n",
    "            epoch_acc += output['acc']\n",
    "\n",
    "            # update the model parameters (weights and biases)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # average the loss and the acc to get the epoch loss and the acc\n",
    "        epoch_loss = epoch_loss / retrain_data['epoch_size']\n",
    "        epoch_acc = epoch_acc / retrain_data['epoch_size']\n",
    "\n",
    "        # output the epoch loss and the epoch acc\n",
    "        logger.info('Loss: %.4f / Acc: %.2f%%' % (epoch_loss, (epoch_acc * 100)))\n",
    "\n",
    "        # increment the number of epochs\n",
    "        epochs_so_far += 1\n",
    "\n",
    "        # tell the scheduler to increment its counter\n",
    "        scheduler.step()\n",
    "\n",
    "    # save a model better than the previous one\n",
    "    torch.save(\n",
    "        model.state_dict(), \n",
    "        path.join(config['results_dir'], 'best_model.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fqo7hAjtwL8j"
   },
   "source": [
    "Let's **retrain** the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3FSHi4wDoQo0"
   },
   "outputs": [],
   "source": [
    "if dataset == 'mini_imagenet':\n",
    "    retrain_data = {\n",
    "        'retrain_x': np.concatenate((train_x, valid_x)),\n",
    "        'retrain_y': np.concatenate((train_y, valid_y)),\n",
    "        'num_way': 20,\n",
    "        'num_shot': 5,\n",
    "        'num_query': 15,\n",
    "        'epoch_size': 100}\n",
    "elif dataset == 'omniglot':    \n",
    "    retrain_data = {\n",
    "        'retrain_x': np.concatenate((train_x, valid_x)),\n",
    "        'retrain_y': np.concatenate((train_y, valid_y)),\n",
    "        'num_way': 60,\n",
    "        'num_shot': 5,\n",
    "        'num_query': 5,\n",
    "        'epoch_size': 100}\n",
    "    \n",
    "retrain_logger = create_logger(path.join(dataset, 'logs'), 'retrain.log')\n",
    "\n",
    "retrain_time = %timeit -n1 -r1 -q -o retrain(model, config, retrain_data, retrain_logger)\n",
    "\n",
    "retrain_logger.info('Time taken: ' + str(retrain_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XTrurUf3tMrm"
   },
   "source": [
    "## Evaluate model on test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nHlhPFjmwcGt"
   },
   "source": [
    "**Function** to evaluate the model on test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xsqqFnSUtP9i"
   },
   "outputs": [],
   "source": [
    "# model -> model structure\n",
    "# test_data -> test set, num_way, num_shot, num_query etc\n",
    "def evaluate_test(model, test_data, logger):\n",
    "    # load the saved model\n",
    "    state_dict = torch.load(path.join(config['results_dir'], 'best_model.pth'))\n",
    "    model.load_state_dict(state_dict)\n",
    "\n",
    "    # set the model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    test_loss = 0.0\n",
    "    test_acc = []\n",
    "\n",
    "    logger.info('> Testing')\n",
    "\n",
    "    # do epoch_size classification tasks to test the model\n",
    "    for episode in trange(test_data['epoch_size']):\n",
    "        # get the episode_dict\n",
    "        episode_dict = extract_episode(\n",
    "            test_data['test_x'], test_data['test_y'], test_data['num_way'], \n",
    "            test_data['num_shot'], test_data['num_query'])\n",
    "\n",
    "        # classify images and get the loss and the acc of the curr episode\n",
    "        _, output = model.set_forward_loss(config['dataset'], episode_dict)\n",
    "\n",
    "        # acumulate the loss and the acc\n",
    "        test_loss += output['loss']\n",
    "        test_acc.append(output['acc'])\n",
    "\n",
    "    # average the loss\n",
    "    test_loss = test_loss / test_data['epoch_size']\n",
    "    \n",
    "    # average the acc\n",
    "    test_acc_avg = sum(test_acc) / test_data['epoch_size']\n",
    "    \n",
    "    # calculate the standard deviation\n",
    "    test_acc_dev = fsum([((x - test_acc_avg) ** 2) for x in test_acc])    \n",
    "    test_acc_dev = (test_acc_dev / (test_data['epoch_size'] - 1)) ** 0.5\n",
    "    \n",
    "    # calculate error considering 95% confidence interval\n",
    "    error = 1.96 * test_acc_dev / (test_data['epoch_size'] ** 0.5)\n",
    "    \n",
    "    # output the test loss and the test acc\n",
    "    logger.info('Loss: %.4f / Acc: %.2f +/- %.2f%%' % (test_loss, test_acc_avg * 100, error * 100))\n",
    "    \n",
    "    return test_acc_avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AEqyZ4kOwhYF"
   },
   "source": [
    "Finally, let's **evaluate** the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yb264DyZt94q"
   },
   "outputs": [],
   "source": [
    "if dataset == 'mini_imagenet':\n",
    "    test_data = {\n",
    "        'test_x': test_x,\n",
    "        'test_y': test_y,\n",
    "        'num_way': 5,\n",
    "        'num_shot': 5,\n",
    "        'num_query': 15,\n",
    "        'epoch_size': 600}\n",
    "elif dataset == 'omniglot':\n",
    "    test_data = {\n",
    "        'test_x': test_x,\n",
    "        'test_y': test_y,\n",
    "        'num_way': 5,\n",
    "        'num_shot': 5,\n",
    "        'num_query': 15,\n",
    "        'epoch_size': 1000}\n",
    "\n",
    "test_logger = create_logger(path.join(dataset, 'logs'), 'test.log')\n",
    "\n",
    "def run_evaluation_n_times(n, model, test_data, logger):\n",
    "    test_acc_list = []\n",
    "    \n",
    "    test_acc = 0\n",
    "    std_dev = 0\n",
    "    \n",
    "    for i in range(n):\n",
    "        output = evaluate_test(model, test_data, logger)\n",
    "        \n",
    "        test_acc_list.append(output)\n",
    "        test_acc += output\n",
    "    \n",
    "    # standard deviation\n",
    "    test_acc = test_acc / n\n",
    "    \n",
    "    # standard deviation\n",
    "    std_dev = fsum([((x - test_acc) ** 2) for x in test_acc_list])    \n",
    "    std_dev = (std_dev / (n - 1)) ** 0.5\n",
    "    \n",
    "    # calculate error considering 95% confidence interval\n",
    "    error = 1.96 * std_dev / (n ** 0.5)\n",
    "    \n",
    "    # output the test loss and the test acc\n",
    "    logger.info('With %i run(s), Acc: %.2f +/- %.2f%%' % (n, test_acc * 100, error * 100))\n",
    "\n",
    "# run evaluation on test 15 times\n",
    "test_time = %timeit -n1 -r1 -q -o run_evaluation_n_times(15, model, test_data, test_logger)\n",
    "\n",
    "test_logger.info('Time taken: ' + str(test_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WPZOXlJbVDea"
   },
   "source": [
    "## Test on specific example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8A1Izs-eWGC2"
   },
   "source": [
    "**Extract** an episode. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ckq1lUSlVKUN"
   },
   "outputs": [],
   "source": [
    "episode_dict = extract_episode(train_x, train_y, num_way = 5, num_shot = 5, num_query = 15)\n",
    "\n",
    "display_episode_images(episode_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xj9_JbnxWRTL"
   },
   "source": [
    "**Load** trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NZSgPKU3WPi4"
   },
   "outputs": [],
   "source": [
    "state_dict = torch.load(path.join(config['results_dir'], 'best_model.pth'))\n",
    "\n",
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z7kUkUJNWiSt"
   },
   "source": [
    "**Test** on example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fw2OFmacWnjS"
   },
   "outputs": [],
   "source": [
    "_, my_output = model.set_forward_loss(config['dataset'], episode_dict)\n",
    "\n",
    "for item in my_output.items():\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ajyUEekbufOD"
   },
   "source": [
    "## Acknowledgements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sH7pFiuvntfI"
   },
   "source": [
    "This project was based on:\n",
    "* [Cyprien Nielly](https://github.com/cnielly/prototypical-networks-omniglot) implementation of Prototypical Networks.\n",
    "\n",
    "* The original implementation, which can be found in [Jake Snell's Github](https://github.com/jakesnell/prototypical-networks).\n",
    "\n",
    "The idea of PNs can be originally found in [Prototypical Networks For Few-shot Learning](https://arxiv.org/abs/1703.05175).\n",
    "\n",
    "It's worth mentioning that using weights in order to calculate the prototypes is in [Improved prototypical networks for few-Shot learning](https://www.sciencedirect.com/science/article/abs/pii/S0167865520302610)."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "gpyltoqgX611",
    "jImaIF9I8lbf",
    "qbJTv_8XBCKF",
    "J2VcplJ3xA-z",
    "1RVjD3LrSfqf",
    "wiJ8ffJxctfD",
    "ssBp1OrZ17oC",
    "7Q8yqvfEnWG5",
    "XTrurUf3tMrm",
    "WPZOXlJbVDea",
    "ajyUEekbufOD"
   ],
   "name": "clean_prototypical_networks.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "environment": {
   "name": "pytorch-gpu.1-8.m65",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-8:m65"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "083079d40f28477494ed7e4369644319": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_2bc3e103637242a2bf5e2e9bef80c5cc",
       "IPY_MODEL_7e29ef302d3e465283d2229189a2b269",
       "IPY_MODEL_a2b94930252b47c5858f36497ce94852"
      ],
      "layout": "IPY_MODEL_bb5b2a083b8c45098c10bc84df06c297"
     }
    },
    "099826cc00db407bb3545bfd00015ebb": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0ad0f7d5f87a4b219c36682d6cf625bb": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1aafcadc64fc4e5f83babd3f77cf6e2f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_1db912dabe304fcf80bb650f61e4a896",
       "IPY_MODEL_f7ce249e92fa4d20b25b6da0f6bd946a",
       "IPY_MODEL_c5d661c6c0f4425e8788187a9972ec0a"
      ],
      "layout": "IPY_MODEL_e0188396ae7b42e58bedb516d3cb4b13"
     }
    },
    "1db912dabe304fcf80bb650f61e4a896": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8059eca7b902455c99b896868c7f9941",
      "placeholder": "​",
      "style": "IPY_MODEL_b555ba895e5b43128bf8b5a689288eea",
      "value": "Reading images from test.pkl: 100%"
     }
    },
    "1eee0765da464d17bed0d545a0ddc9df": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_099826cc00db407bb3545bfd00015ebb",
      "placeholder": "​",
      "style": "IPY_MODEL_fda7454d60424b519c64025744043b42",
      "value": "Reading images from train.pkl: 100%"
     }
    },
    "265c63ed3b724b3490cd6a08e36d2148": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2bc3e103637242a2bf5e2e9bef80c5cc": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e32b2f09018745d7918f40a33eb5441c",
      "placeholder": "​",
      "style": "IPY_MODEL_0ad0f7d5f87a4b219c36682d6cf625bb",
      "value": "Reading images from valid.pkl: 100%"
     }
    },
    "371bc0d2a0274f22af27b0cb23fbb87e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_1eee0765da464d17bed0d545a0ddc9df",
       "IPY_MODEL_ec40b90019de4641ad0bf329fd665050",
       "IPY_MODEL_fa6a285b2f524ae1b740461b49b76c65"
      ],
      "layout": "IPY_MODEL_9c128fc5b1bc47a1aabf1a79634ffd19"
     }
    },
    "53a874f57fe8428998bcb6fbc020a1ff": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5f185babc81b4b5b8f2e00afe759167f": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7e29ef302d3e465283d2229189a2b269": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_265c63ed3b724b3490cd6a08e36d2148",
      "max": 16,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_aa779ef27a8d4cc8884b3b5d4309899d",
      "value": 16
     }
    },
    "8059eca7b902455c99b896868c7f9941": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "90cea8909566451595c451d1262e7508": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "926802ff8fcb41fc8f7cc27badf40878": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "95f769bcafb740f8a5c9f3eec65de53f": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9c128fc5b1bc47a1aabf1a79634ffd19": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a07c1badcd394778b32b2b06c693e3cf": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "a2b94930252b47c5858f36497ce94852": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fa93e90d48df41f38cad9e13d1d8872e",
      "placeholder": "​",
      "style": "IPY_MODEL_be13b4660eef4410a954d51de86e82ab",
      "value": " 16/16 [00:00&lt;00:00, 136.18it/s]"
     }
    },
    "aa779ef27a8d4cc8884b3b5d4309899d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "b1ebf174c7be42c28972f76ae8478cf6": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b555ba895e5b43128bf8b5a689288eea": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "bb5b2a083b8c45098c10bc84df06c297": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "be13b4660eef4410a954d51de86e82ab": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c5d661c6c0f4425e8788187a9972ec0a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5f185babc81b4b5b8f2e00afe759167f",
      "placeholder": "​",
      "style": "IPY_MODEL_f72bee58c9964049a186da3ebdca2927",
      "value": " 20/20 [00:00&lt;00:00, 139.80it/s]"
     }
    },
    "e0188396ae7b42e58bedb516d3cb4b13": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e32b2f09018745d7918f40a33eb5441c": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ec40b90019de4641ad0bf329fd665050": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_53a874f57fe8428998bcb6fbc020a1ff",
      "max": 64,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_926802ff8fcb41fc8f7cc27badf40878",
      "value": 64
     }
    },
    "f72bee58c9964049a186da3ebdca2927": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f7ce249e92fa4d20b25b6da0f6bd946a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_95f769bcafb740f8a5c9f3eec65de53f",
      "max": 20,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_a07c1badcd394778b32b2b06c693e3cf",
      "value": 20
     }
    },
    "fa6a285b2f524ae1b740461b49b76c65": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_90cea8909566451595c451d1262e7508",
      "placeholder": "​",
      "style": "IPY_MODEL_b1ebf174c7be42c28972f76ae8478cf6",
      "value": " 64/64 [00:00&lt;00:00, 92.88it/s]"
     }
    },
    "fa93e90d48df41f38cad9e13d1d8872e": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fda7454d60424b519c64025744043b42": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
